{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom_implementation_of_TF-IDF_vectorizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnV82tg1xLi0"
      },
      "source": [
        "### Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUsYm9wjxLi1"
      },
      "source": [
        "#SkLearn Collection of string documents\n",
        "\n",
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLwmFZfKxLi4"
      },
      "source": [
        "### SkLearn Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np4dfQOkxLi4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "skl_output = vectorizer.transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Om8YpYxLi6",
        "outputId": "1024b83a-9f97-45ec-b70a-c1f141e8ecbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# sklearn feature names, they are sorted in alphabetic order by default.\n",
        "\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTKplK96xLi-",
        "outputId": "88d0095d-1b87-419e-a7d8-bd5309bff647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
        "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
        "\n",
        "print(vectorizer.idf_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
            " 1.         1.91629073 1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CTiWHygxLjA",
        "outputId": "aca71162-49a8-4310-9fab-089eace9dfd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
        "\n",
        "skl_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDKEpbA-xLjD",
        "outputId": "0cb1c414-3843-4cf2-946c-5a6f7901f812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# Here the output is a sparse matrix\n",
        "\n",
        "print(skl_output[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 8)\t0.38408524091481483\n",
            "  (0, 6)\t0.38408524091481483\n",
            "  (0, 3)\t0.38408524091481483\n",
            "  (0, 2)\t0.5802858236844359\n",
            "  (0, 1)\t0.46979138557992045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QWo34hexLjF",
        "outputId": "ee46ecbe-dbfb-486e-bb60-8a273f8c1168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# sklearn tfidf values for first line of the above corpus.\n",
        "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
        "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
        "\n",
        "print(skl_output[0].toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfIwx5LzxLjI"
      },
      "source": [
        "## Custom implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjuCcJwXxLjJ"
      },
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(dataset):\n",
        "    uni_words = set()\n",
        "    idf_dict={}\n",
        "    N=len(dataset)\n",
        "    if isinstance(dataset, (list,)):\n",
        "      for x in dataset:\n",
        "        for y in x.split():\n",
        "          if len(y)<2:\n",
        "            continue\n",
        "          uni_words.add(y)\n",
        "      uni_words = sorted(list(uni_words))\n",
        "      vocab = {j:i for i,j in enumerate(uni_words)}\n",
        "      for i in uni_words:\n",
        "        cnt=0\n",
        "        for sents in dataset:\n",
        "          if i in sents.split():\n",
        "            cnt=cnt+1\n",
        "            idf_dict[i]=(math.log((1+N)/(cnt+1)))+1\n",
        "    return vocab, idf_dict\n",
        " \n",
        "      "
      ],
      "metadata": {
        "id": "VFgOz_wqKZS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, idf_of_vocab=fit(corpus)"
      ],
      "metadata": {
        "id": "BoqpMQirNsex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(vocab))\n",
        "print(type(idf_of_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY5_jXPwurz6",
        "outputId": "8badcb62-9f32-4080-ad4d-f2056434cec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]\n"
      ],
      "metadata": {
        "id": "cYCJjDgz9d1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(idf_of_vocab.keys()))"
      ],
      "metadata": {
        "id": "TYRfX517EexI",
        "outputId": "da403953-1916-4357-dfd2-3805cca6d6eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(idf_of_vocab.values()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbxZwyrxX4hw",
        "outputId": "c0d137bd-edc3-4b21-f84f-763e8b5f4ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_idx=(sorted(list(idf_of_vocab.values()),reverse=True))\n",
        "for i in sorted_idx:\n",
        "  print(list(idf_of_vocab.keys())[list(idf_of_vocab.values()).index(i)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4NoE3U6CsYi",
        "outputId": "19a24a62-5d15-48d0-ec71-924cd3b53c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and\n",
            "and\n",
            "and\n",
            "and\n",
            "first\n",
            "document\n",
            "is\n",
            "is\n",
            "is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(dataset,vocabulary,idf_values):\n",
        "     sparse_matrix= csr_matrix( (len(dataset), len(vocabulary)), dtype=np.float64)\n",
        "     for row  in range(0,len(dataset)):\n",
        "       number_of_words_in_sentence=Counter(dataset[row].split())\n",
        "       for word in dataset[row].split():\n",
        "           if word in  list(vocabulary.keys()):\n",
        "               tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
        "               sparse_matrix[row,vocabulary[word]]=tf_idf_value\n",
        "     print(\"NORM FORM\\n\",normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False))\n",
        "     output =normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
        "     return output"
      ],
      "metadata": {
        "id": "Wokc65R1aDN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_final_output=transform(corpus,vocab,idf_of_vocab)\n",
        "print(custom_final_output.shape) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbYH69MMUL50",
        "outputId": "ff237610-eadf-4b1a-ba77-0319a03b1149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NORM FORM\n",
            "   (0, 1)\t0.4697913855799205\n",
            "  (0, 2)\t0.580285823684436\n",
            "  (0, 3)\t0.3840852409148149\n",
            "  (0, 6)\t0.3840852409148149\n",
            "  (0, 8)\t0.3840852409148149\n",
            "  (1, 1)\t0.6876235979836937\n",
            "  (1, 3)\t0.2810886740337529\n",
            "  (1, 5)\t0.5386476208856762\n",
            "  (1, 6)\t0.2810886740337529\n",
            "  (1, 8)\t0.2810886740337529\n",
            "  (2, 0)\t0.511848512707169\n",
            "  (2, 3)\t0.267103787642168\n",
            "  (2, 4)\t0.511848512707169\n",
            "  (2, 6)\t0.267103787642168\n",
            "  (2, 7)\t0.511848512707169\n",
            "  (2, 8)\t0.267103787642168\n",
            "  (3, 1)\t0.4697913855799205\n",
            "  (3, 2)\t0.580285823684436\n",
            "  (3, 3)\t0.3840852409148149\n",
            "  (3, 6)\t0.3840852409148149\n",
            "  (3, 8)\t0.3840852409148149\n",
            "(4, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:84: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_intXint(row, col, x.flat[0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(custom_final_output[0].toarray())"
      ],
      "metadata": {
        "id": "rnJfzi8D_EFi",
        "outputId": "24c8146b-f38e-40bb-ccb7-8eadbddcb00c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMxBmVZExLjK"
      },
      "source": [
        "## Here, we have a file 'cleaned_strings'. We print out the top 50 IDF values from the text corpus in it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHxPLlwNxLjL",
        "outputId": "013360de-74e1-4be5-9eab-273f3ec3ccea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Below is the code to load the cleaned_strings pickle file provided\n",
        "# Here corpus is of list type\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pickle\n",
        "with open('/content/drive/My Drive/cleaned_strings', 'rb') as f:\n",
        "    corpus = pickle.load(f)\n",
        "    \n",
        "# printing the length of the corpus loaded\n",
        "print(\"Number of documents in corpus = \",len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Number of documents in corpus =  746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log10\n",
        "\n",
        "def idf(dataset,word):\n",
        "  count =0\n",
        "  for row in dataset:\n",
        "    if word in row:\n",
        "      count = count+1\n",
        "  return count\n",
        "\n",
        "def fit(dataset):\n",
        "  unique_words=[]\n",
        "  IDF_val=[]\n",
        "  for row in dataset:\n",
        "    for word in row.split(\" \"):\n",
        "      if len(word)>=2 and word not in unique_words:\n",
        "        unique_words.append(word)\n",
        "\n",
        "  for word in unique_words:\n",
        "    val=log10(len(dataset)/idf(dataset,word))\n",
        "    IDF_val.append(val)\n",
        "  for i in range(len(IDF_val)):\n",
        "    for j in range(len(IDF_val)):\n",
        "      if IDF_val[j] < IDF_val[i]:\n",
        "        t_val = IDF_val[j] \n",
        "        IDF_val[j] = IDF_val[i]\n",
        "        IDF_val[i] = t_val\n",
        "\n",
        "        t_word = unique_words[j]\n",
        "        unique_words[j] = unique_words[i]\n",
        "        unique_words[i] = t_word\n",
        "\n",
        "  vocab = {j:i for i,j in enumerate(unique_words[:50])}\n",
        "  return vocab\n",
        "\n",
        "vocab = fit(corpus)\n",
        "print(vocab)                    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj5rjZeRLeP7",
        "outputId": "ddb7b89c-3076-4566-cdd0-b334f1ca0a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'aimless': 0, 'distressed': 1, 'drifting': 2, 'nearly': 3, 'attempting': 4, 'artiness': 5, 'gerardo': 6, 'emptiness': 7, 'messages': 8, 'buffet': 9, 'science': 10, 'teacher': 11, 'owls': 12, 'florida': 13, 'muppets': 14, 'overdue': 15, 'screenplay': 16, 'post': 17, 'practically': 18, 'structure': 19, 'tightly': 20, 'constructed': 21, 'vitally': 22, 'occurs': 23, 'content': 24, 'dozen': 25, 'highest': 26, 'superlative': 27, 'require': 28, 'puzzle': 29, 'solving': 30, 'fit': 31, 'pulls': 32, 'punches': 33, 'graphics': 34, 'insane': 35, 'massive': 36, 'unlockable': 37, 'properly': 38, 'rocks': 39, 'doomed': 40, 'conception': 41, 'minor': 42, 'changing': 43, 'confirm': 44, 'generic': 45, 'managed': 46, 'exaggerating': 47, 'trailer': 48, 'carrell': 49}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(dataset,vocab):\n",
        "  rows=[]\n",
        "  columns =[]\n",
        "  values=[]\n",
        "  tf_val=[]\n",
        "  idf_val=[]\n",
        "  for idx,row in enumerate(dataset):\n",
        "    word_freq = dict(Counter(row.split()))\n",
        "    for word,freq in word_freq.items():\n",
        "      if len(word)<2:\n",
        "        continue\n",
        "      col_index = vocab.get(word,-1)\n",
        "      if col_index != -1:\n",
        "        rows.append(idx)\n",
        "        columns.append(col_index)\n",
        "        val = (freq/len(row.split()))*(log10(len(dataset)/idf(dataset,word)))\n",
        "        values.append(val)\n",
        "\n",
        "  return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab)))        "
      ],
      "metadata": {
        "id": "GDkfGFZpLefZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = fit(corpus)\n",
        "tf_idf_vect = transform(corpus, vocab)\n",
        "print(tf_idf_vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1lRKkBFOx5G",
        "outputId": "47a210d4-caf4-4a7f-922a-c3e1a74ab06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t0.3590923534340836\n",
            "  (0, 1)\t0.3590923534340836\n",
            "  (0, 2)\t0.3590923534340836\n",
            "  (1, 3)\t0.3191932030525187\n",
            "  (2, 4)\t0.15119678039329834\n",
            "  (2, 5)\t0.15119678039329834\n",
            "  (4, 6)\t0.2872738827472669\n",
            "  (5, 7)\t0.2872738827472669\n",
            "  (7, 8)\t0.3191932030525187\n",
            "  (9, 9)\t0.47878980457877807\n",
            "  (9, 10)\t0.47878980457877807\n",
            "  (9, 11)\t0.47878980457877807\n",
            "  (10, 12)\t0.9575796091575561\n",
            "  (11, 13)\t0.3590923534340836\n",
            "  (12, 14)\t0.7181847068681672\n",
            "  (16, 15)\t0.2209799098055899\n",
            "  (17, 16)\t0.20519563053376202\n",
            "  (17, 17)\t0.20519563053376202\n",
            "  (18, 18)\t0.41039126106752405\n",
            "  (19, 19)\t0.006543824208365988\n",
            "  (19, 20)\t0.006543824208365988\n",
            "  (19, 21)\t0.006543824208365988\n",
            "  (19, 22)\t0.006543824208365988\n",
            "  (19, 23)\t0.006543824208365988\n",
            "  (19, 24)\t0.006543824208365988\n",
            "  (19, 25)\t0.006543824208365988\n",
            "  (19, 26)\t0.006543824208365988\n",
            "  (19, 27)\t0.006543824208365988\n",
            "  (19, 28)\t0.006543824208365988\n",
            "  (19, 29)\t0.006543824208365988\n",
            "  (19, 30)\t0.006543824208365988\n",
            "  (19, 31)\t0.013087648416731976\n",
            "  (19, 32)\t0.006543824208365988\n",
            "  (19, 33)\t0.006543824208365988\n",
            "  (19, 34)\t0.019631472625097963\n",
            "  (19, 35)\t0.006543824208365988\n",
            "  (19, 36)\t0.019631472625097963\n",
            "  (19, 37)\t0.006543824208365988\n",
            "  (19, 38)\t0.006543824208365988\n",
            "  (19, 39)\t0.006543824208365988\n",
            "  (19, 40)\t0.006543824208365988\n",
            "  (19, 41)\t0.006543824208365988\n",
            "  (19, 42)\t0.006543824208365988\n",
            "  (19, 43)\t0.006543824208365988\n",
            "  (19, 44)\t0.006543824208365988\n",
            "  (19, 45)\t0.006543824208365988\n",
            "  (19, 46)\t0.006543824208365988\n",
            "  (19, 47)\t0.006543824208365988\n",
            "  (19, 48)\t0.006543824208365988\n",
            "  (19, 49)\t0.006543824208365988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Here, we have compared to the custom implementation to sklearn's.\n",
        "*   We have also printed out the top 50 idf values.\n",
        "\n"
      ],
      "metadata": {
        "id": "QI-AMwQ0WsIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DZD_LdseOyGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}